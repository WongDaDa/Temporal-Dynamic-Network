{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e85879e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import pickle\n",
    "import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9d1f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalDependency(nn.Module):\n",
    "    def __init__(self,data_size,layer_size,layer_num,batch_size,cuda):\n",
    "        super(TemporalDependency,self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = layer_size\n",
    "        self.layer_num = layer_num\n",
    "        self.cuda = cuda\n",
    "        self.data_size = data_size\n",
    "        self.Dependency = nn.LSTM(batch_first=True,\n",
    "                                  input_size=self.data_size,\n",
    "                                  num_layers=self.layer_num,\n",
    "                                  hidden_size=self.hidden_size)\n",
    "    def init_hidden(self,cuda):\n",
    "        if cuda:\n",
    "            return (Variable(torch.zeros(self.layer_num,\n",
    "                                         self.batch_size,\n",
    "                                         self.Dependency.hidden_size),\n",
    "                             requires_grad=True).cuda(),\n",
    "                    Variable(torch.zeros(self.layer_num,\n",
    "                                         self.batch_size,\n",
    "                                         self.Dependency.hidden_size),\n",
    "                             requires_grad=True).cuda())\n",
    "        else:\n",
    "            return (Variable(torch.zeros(self.layer_num,\n",
    "                                         self.batch_size,\n",
    "                                         self.Dependency.hidden_size),\n",
    "                             requires_grad=True),\n",
    "                    Variable(torch.zeros(self.layer_num,\n",
    "                                         self.batch_size,\n",
    "                                         self.Dependency.hidden_size),\n",
    "                             requires_grad=True))\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        hidden = self.init_hidden(self.cuda)\n",
    "        output, _ = self.Dependency(inputs,hidden)\n",
    "        return output\n",
    "    \n",
    "class ContentBasedAttention(nn.Module):\n",
    "    def __init__(self,period_length,data_length,layer_size,batch_size,cuda):\n",
    "        super(ContentBasedAttention,self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = layer_size\n",
    "        self.cuda = cuda\n",
    "        self.period_length = period_length\n",
    "        self.data_length = data_length\n",
    "        self.Attention = nn.LSTMCell(input_size=self.data_length,\n",
    "                                    hidden_size=layer_size)\n",
    "        \n",
    "        self.Wh = torch.nn.Parameter(torch.zeros(batch_size,layer_size,layer_size))\n",
    "        self.Wx = torch.nn.Parameter(torch.zeros(batch_size,layer_size,layer_size))\n",
    "        self.bx = torch.nn.Parameter(torch.zeros(batch_size,layer_size,1))\n",
    "        self.v = torch.nn.Parameter(torch.zeros(batch_size,layer_size))\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "    def init_hidden(self,cuda):\n",
    "        if cuda:\n",
    "            return (Variable(torch.zeros(self.batch_size,\n",
    "                                         self.Attention.hidden_size),\n",
    "                             requires_grad=True).cuda(),\n",
    "                    Variable(torch.zeros(self.batch_size,\n",
    "                                         self.Attention.hidden_size),\n",
    "                             requires_grad=True).cuda())\n",
    "        else:\n",
    "            return (Variable(torch.zeros(self.batch_size,\n",
    "                                         self.Attention.hidden_size),\n",
    "                             requires_grad=True),\n",
    "                    Variable(torch.zeros(self.batch_size,\n",
    "                                         self.Attention.hidden_size),\n",
    "                             requires_grad=True))\n",
    "    \n",
    "    def forward(self,inputs,target):\n",
    "        hidden = self.init_hidden(self.cuda)\n",
    "        score = []\n",
    "        h_ps = []\n",
    "        for p in range(self.period_length):\n",
    "            h_p, cell = self.Attention(inputs[p],hidden)\n",
    "            scoring = torch.bmm(self.v.unsqueeze(1),(torch.tanh(\n",
    "                torch.bmm(self.Wh,h_p.view(h_p.shape[0],h_p.shape[1],1))\n",
    "                +torch.bmm(self.Wx,target.view(target.shape[0],target.shape[1],1))\n",
    "                +self.bx))).squeeze()\n",
    "            score.append(scoring)\n",
    "            h_ps.append(h_p)\n",
    "            hidden = tuple((h_p,cell))\n",
    "        score = torch.stack(score)\n",
    "        h_ps = torch.stack(h_ps).permute(1,0,2)\n",
    "        softmax_s = self.softmax(score).permute(1,0)\n",
    "        represent = softmax_s.unsqueeze(1).bmm(h_ps)\n",
    "        \n",
    "        return represent\n",
    "    \n",
    "class Regressor(nn.Module):\n",
    "    def __init__(self,input_shape,output_shape,cuda):\n",
    "        super(Regressor,self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.cuda = cuda\n",
    "        self.layer = nn.Linear(self.input_shape,self.output_shape)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        outputs = self.layer(inputs)\n",
    "        result = self.activation(outputs)\n",
    "        return result\n",
    "    \n",
    "class TemporalDynamicNetwork(nn.Module):\n",
    "    def __init__(self,data_size,layer_size,period_length,term_length,batch_size,PSAM,cuda):\n",
    "        super(TemporalDynamicNetwork,self).__init__()\n",
    "        self.data_size = data_size\n",
    "        self.layer_size = layer_size\n",
    "        self.period_length = period_length\n",
    "        self.term_length = term_length\n",
    "        self.batch_size = batch_size\n",
    "        self.PSAM = PSAM\n",
    "        self.cuda = cuda\n",
    "        \n",
    "        self.ShortTerm = TemporalDependency(data_size=data_size,layer_size=layer_size,layer_num=1,batch_size=batch_size,cuda=cuda)\n",
    "        \n",
    "        self.LongTerm = TemporalDependency(data_size=layer_size,layer_size=layer_size,layer_num=1,batch_size=batch_size,cuda=cuda)\n",
    "        \n",
    "        self.Attention = ContentBasedAttention(period_length=term_length,data_length=data_size,layer_size=layer_size,batch_size=batch_size,cuda=cuda)\n",
    "        \n",
    "        self.regressor = Regressor(input_shape=layer_size,output_shape=data_size,cuda=cuda)\n",
    "        \n",
    "    def forward(self,att,x,y):\n",
    "        short_rep = self.ShortTerm.forward(x)[:,-1,:]\n",
    "        if self.PSAM:\n",
    "            att_rep = []\n",
    "            for a in att:\n",
    "                attention = self.Attention.forward(a,short_rep)\n",
    "                att_rep.append(attention)\n",
    "            att_rep = torch.stack(att_rep).permute(1,0,2,3)\n",
    "            long_rep = self.LongTerm.forward(att_rep.squeeze())[:,-1,:]\n",
    "            pred = self.regressor.forward(short_rep+long_rep)\n",
    "        else:\n",
    "            pred = self.regressor.forward(short_rep)\n",
    "        \n",
    "        loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "        loss = loss_fn(y, pred)\n",
    "        \n",
    "        return loss,pred.data.cpu().numpy()\n",
    "    \n",
    "def Train(total_train_batches,model,optimizer,dataLoader):\n",
    "    total_loss = 0\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "    for i in range(total_train_batches):\n",
    "        att,x,y = dataLoader.sample_train_batch()\n",
    "        att = Variable(torch.from_numpy(att)).float()\n",
    "        att = att.permute(1,2,0,3)\n",
    "        x = Variable(torch.from_numpy(x)).float()\n",
    "        y = Variable(torch.from_numpy(y),requires_grad=False).float()\n",
    "\n",
    "        loss,pred = model.forward(att.cuda(),x.cuda(),y.cuda())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.data.cpu().numpy()/total_train_batches\n",
    "    return total_loss,pred\n",
    "\n",
    "def Test(total_batches,model,optimizer,dataLoader):\n",
    "    total_loss = 0\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "    for i in range(total_batches):\n",
    "        att,x,y = dataLoader.sample_test_batch()\n",
    "        att = Variable(torch.from_numpy(att)).float()\n",
    "        att = att.permute(1,2,0,3)\n",
    "        x = Variable(torch.from_numpy(x)).float()\n",
    "        y = Variable(torch.from_numpy(y),requires_grad=False).float()\n",
    "\n",
    "        loss,pred = model.forward(att.cuda(),x.cuda(),y.cuda())\n",
    "        total_loss += loss.data.cpu().numpy()/total_batches\n",
    "\n",
    "    return total_loss,pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121c85be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NYCLoader():\n",
    "    def __init__(self,batch_size,period_length,term_length,interval_length,seed,data_source):\n",
    "        self.batch_size = batch_size\n",
    "        self.period_length = period_length\n",
    "        self.term_length = term_length\n",
    "        self.interval_length = interval_length\n",
    "        self.seed = seed\n",
    "        self.source = data_source\n",
    "        self.pointer = {'train': 0, 'test': 0}\n",
    "        np.random.seed(self.seed)\n",
    "        \n",
    "    def init_data(self):\n",
    "        self.x = np.random.randint(10)\n",
    "        self.y = np.random.randint(20)\n",
    "        print('selected region: ({},{})'.format(self.x,self.y))\n",
    "        if self.source=='taxi':\n",
    "            self.train_data = np.load(open('volume_train.npz','rb'))['volume'][:,self.x,self.y,:]\n",
    "            \n",
    "            self.test_data = np.load(open('volume_test.npz','rb'))['volume'][:,self.x,self.y,:]\n",
    "            \n",
    "        else:\n",
    "            self.train_data = np.load(open('bike_volume_train.npz','rb'))['volume'][:,self.x,self.y,:]\n",
    "            \n",
    "            self.test_data = np.load(open('bike_volume_train.npz','rb'))['volume'][:,self.x,self.y,:]\n",
    "        \n",
    "        train_data_length = 1920 - self.interval_length - int(48*self.period_length) - int(self.term_length/2) - 1\n",
    "        test_data_length = 960 - self.interval_length - int(48*self.period_length) - int(self.term_length/2) - 1\n",
    "        \n",
    "        return train_data_length,test_data_length\n",
    "        \n",
    "    def sample_train_batch(self):\n",
    "        available_start = int(48*self.period_length) + int(self.term_length/2)\n",
    "        available_end = 1920 - self.interval_length - 1\n",
    "        rand_samples = np.random.randint(available_start,available_end,self.batch_size)\n",
    "        \n",
    "        attention_samples = []\n",
    "        prediction_samples = []\n",
    "        result = []\n",
    "        \n",
    "        for i in rand_samples:\n",
    "            prediction_samples.append(self.train_data[i:i+self.interval_length])\n",
    "            result.append(self.train_data[i+self.interval_length+1])\n",
    "            attention =[]\n",
    "            for d in range(1,self.period_length+1):\n",
    "                attention.append(self.train_data[i-int(48*d)-int(self.term_length/2):i-int(48*d)+int(self.term_length/2)+1])\n",
    "                attention.reverse()\n",
    "            \n",
    "            attention_samples.append(attention)\n",
    "                \n",
    "        attention_samples = np.array(attention_samples)\n",
    "        prediction_samples = np.array(prediction_samples)\n",
    "        result = np.array(result)\n",
    "                \n",
    "        return attention_samples,prediction_samples,result\n",
    "    \n",
    "    def sample_test_batch(self):\n",
    "        available_start = int(48*self.period_length) + int(self.term_length/2)\n",
    "        available_end = 960 - self.interval_length - 1\n",
    "        samples = np.arange(available_start + self.pointer['test'],available_start + self.pointer['test'] + self.batch_size)\n",
    "        self.pointer['test'] += self.batch_size\n",
    "        if available_start + self.pointer['test'] + self.batch_size > available_end:\n",
    "            self.pointer['test'] = 0\n",
    "        \n",
    "        attention_samples = []\n",
    "        prediction_samples = []\n",
    "        result = []\n",
    "        \n",
    "        for i in samples:\n",
    "            prediction_samples.append(self.test_data[i:i+self.interval_length])\n",
    "            result.append(self.test_data[i+self.interval_length+1])\n",
    "            attention =[]\n",
    "            for d in range(1,self.period_length+1):\n",
    "                attention.append(self.test_data[i-int(48*d)-int(self.term_length/2):i-int(48*d)+int(self.term_length/2)+1])\n",
    "                attention.reverse()\n",
    "            \n",
    "            attention_samples.append(attention)\n",
    "            \n",
    "        attention_samples = np.array(attention_samples)\n",
    "        prediction_samples = np.array(prediction_samples)\n",
    "        result = np.array(result)\n",
    "                \n",
    "        return attention_samples,prediction_samples,result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d310c480",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElectricityLoader():\n",
    "    def __init__(self,batch_size,period_length,term_length,interval_length,seed):\n",
    "        self.batch_size = batch_size\n",
    "        self.period_length = period_length\n",
    "        self.term_length = term_length\n",
    "        self.interval_length = interval_length\n",
    "        self.seed = seed\n",
    "        self.pointer = {'train': 0, 'test': 0}\n",
    "        np.random.seed(self.seed)\n",
    "    \n",
    "    def init_data(self):\n",
    "        self.user = np.random.randint(370)\n",
    "        print('selected user: {}'.format(self.user))\n",
    "        \n",
    "        raw_data = pd.read_table('Electricity.txt',sep=';',index_col=0).iloc[-35041:-1,self.user].values\n",
    "        data = np.array([item.replace(',', '.') for item in raw_data], dtype=np.float32)\n",
    "        data = np.array([np.sum(data[i:i+4]) for i in range(0,len(data),4)])\n",
    "        \n",
    "        self.train_data = data[:int(len(data)*0.7)]\n",
    "        self.train_data = self.train_data.reshape(self.train_data.shape[0],1)\n",
    "        \n",
    "        self.test_data = data[int(len(data)*0.7):]\n",
    "        self.test_data = self.test_data.reshape(self.test_data.shape[0],1)\n",
    "        \n",
    "        train_data_length = 6132 - self.interval_length - int(24*self.period_length) - int(self.term_length/2) - 1\n",
    "        test_data_length = 2628 - self.interval_length - int(24*self.period_length) - int(self.term_length/2) - 1\n",
    "        \n",
    "        return train_data_length,test_data_length\n",
    "    \n",
    "    def sample_train_batch(self):\n",
    "        available_start = int(24*self.period_length) + int(self.term_length/2)\n",
    "        available_end = 6132 - self.interval_length - 1\n",
    "        rand_samples = np.random.randint(available_start,available_end,self.batch_size)\n",
    "        \n",
    "        attention_samples = []\n",
    "        prediction_samples = []\n",
    "        result = []\n",
    "        \n",
    "        for i in rand_samples:\n",
    "            prediction_samples.append(self.train_data[i:i+self.interval_length])\n",
    "            result.append(self.train_data[i+self.interval_length+1])\n",
    "            attention =[]\n",
    "            for d in range(1,self.period_length+1):\n",
    "                attention.append(self.train_data[i-int(24*d)-int(self.term_length/2):i-int(24*d)+int(self.term_length/2)+1])\n",
    "                attention.reverse()\n",
    "            \n",
    "            attention_samples.append(attention)\n",
    "                \n",
    "        attention_samples = (np.array(attention_samples)-np.min(attention_samples))/(np.max(attention_samples)-np.min(attention_samples))\n",
    "        prediction_samples = (np.array(prediction_samples)-np.min(prediction_samples))/(np.max(prediction_samples)-np.min(prediction_samples))\n",
    "        result = np.array(result)\n",
    "                \n",
    "        return attention_samples,prediction_samples,result\n",
    "    \n",
    "    def sample_test_batch(self):\n",
    "        available_start = int(24*self.period_length) + int(self.term_length/2)\n",
    "        available_end = 2628 - self.interval_length - 1\n",
    "        samples = np.arange(available_start + self.pointer['test'],available_start + self.pointer['test'] + self.batch_size)\n",
    "        self.pointer['test'] += self.batch_size\n",
    "        if available_start + self.pointer['test'] + self.batch_size > available_end:\n",
    "            self.pointer['test'] = 0\n",
    "        \n",
    "        attention_samples = []\n",
    "        prediction_samples = []\n",
    "        result = []\n",
    "        \n",
    "        for i in samples:\n",
    "            prediction_samples.append(self.test_data[i:i+self.interval_length])\n",
    "            result.append(self.test_data[i+self.interval_length+1])\n",
    "            attention =[]\n",
    "            for d in range(1,self.period_length+1):\n",
    "                attention.append(self.test_data[i-int(24*d)-int(self.term_length/2):i-int(24*d)+int(self.term_length/2)+1])\n",
    "                attention.reverse()\n",
    "            \n",
    "            attention_samples.append(attention)\n",
    "                \n",
    "        attention_samples = (np.array(attention_samples)-np.min(attention_samples))/(np.max(attention_samples)-np.min(attention_samples))\n",
    "        prediction_samples = (np.array(prediction_samples)-np.min(prediction_samples))/(np.max(prediction_samples)-np.min(prediction_samples))\n",
    "        result = np.array(result)\n",
    "                \n",
    "        return attention_samples,prediction_samples,result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab721e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SolarLoader():\n",
    "    def __init__(self,batch_size,period_length,term_length,interval_length,seed):\n",
    "        self.batch_size = batch_size\n",
    "        self.period_length = period_length\n",
    "        self.term_length = term_length\n",
    "        self.interval_length = interval_length\n",
    "        self.seed = seed\n",
    "        self.pointer = {'train': 0, 'test': 0}\n",
    "        np.random.seed(self.seed)\n",
    "        \n",
    "    def init_data(self):\n",
    "        raw_data = pd.read_csv('NY_Solar.csv',index_col=0).values\n",
    "        data = np.array([np.sum(raw_data[i:i+6]) for i in range(0,len(raw_data),6)])\n",
    "        \n",
    "        self.train_data = data[:int(len(data)*0.7)]\n",
    "        self.train_data = self.train_data.reshape(self.train_data.shape[0],1)\n",
    "        \n",
    "        self.test_data = data[int(len(data)*0.7):]\n",
    "        self.test_data = self.test_data.reshape(self.test_data.shape[0],1)\n",
    "        \n",
    "        train_data_length = 12264 - self.interval_length - int(48*self.period_length) - int(self.term_length/2) - 1\n",
    "        test_data_length = 5256 - self.interval_length - int(48*self.period_length) - int(self.term_length/2) - 1\n",
    "        \n",
    "        return train_data_length,test_data_length\n",
    "    \n",
    "    def sample_train_batch(self):\n",
    "        available_start = int(48*self.period_length) + int(self.term_length/2)\n",
    "        available_end = 12264 - self.interval_length - 1\n",
    "        rand_samples = np.random.randint(available_start,available_end,self.batch_size)\n",
    "        \n",
    "        attention_samples = []\n",
    "        prediction_samples = []\n",
    "        result = []\n",
    "        \n",
    "        for i in rand_samples:\n",
    "            prediction_samples.append(self.train_data[i:i+self.interval_length])\n",
    "            result.append(self.train_data[i+self.interval_length+1])\n",
    "            attention =[]\n",
    "            for d in range(1,self.period_length+1):\n",
    "                attention.append(self.train_data[i-int(48*d)-int(self.term_length/2):i-int(48*d)+int(self.term_length/2)+1])\n",
    "                attention.reverse()\n",
    "            \n",
    "            attention_samples.append(attention)\n",
    "                \n",
    "        attention_samples = (np.array(attention_samples)-np.min(attention_samples))/(np.max(attention_samples)-np.min(attention_samples))\n",
    "        prediction_samples = (np.array(prediction_samples)-np.min(prediction_samples))/(np.max(prediction_samples)-np.min(prediction_samples))\n",
    "        result = np.array(result)\n",
    "                \n",
    "        return attention_samples,prediction_samples,result\n",
    "    \n",
    "    def sample_test_batch(self):\n",
    "        available_start = int(48*self.period_length) + int(self.term_length/2)\n",
    "        available_end = 5256 - self.interval_length - 1\n",
    "        samples = np.arange(available_start + self.pointer['test'],available_start + self.pointer['test'] + self.batch_size)\n",
    "        self.pointer['test'] += self.batch_size\n",
    "        if available_start + self.pointer['test'] + self.batch_size > available_end:\n",
    "            self.pointer['test'] = 0\n",
    "        \n",
    "        attention_samples = []\n",
    "        prediction_samples = []\n",
    "        result = []\n",
    "        \n",
    "        for i in samples:\n",
    "            prediction_samples.append(self.test_data[i:i+self.interval_length])\n",
    "            result.append(self.test_data[i+self.interval_length+1])\n",
    "            attention =[]\n",
    "            for d in range(1,self.period_length+1):\n",
    "                attention.append(self.test_data[i-int(48*d)-int(self.term_length/2):i-int(48*d)+int(self.term_length/2)+1])\n",
    "                attention.reverse()\n",
    "            \n",
    "            attention_samples.append(attention)\n",
    "                \n",
    "        attention_samples = (np.array(attention_samples)-np.min(attention_samples))/(np.max(attention_samples)-np.min(attention_samples))\n",
    "        prediction_samples = (np.array(prediction_samples)-np.min(prediction_samples))/(np.max(prediction_samples)-np.min(prediction_samples))\n",
    "        result = np.array(result)\n",
    "                \n",
    "        return attention_samples,prediction_samples,result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade2aa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NYC = NYCLoader(batch_size=64,period_length=7,term_length=5,interval_length=7,seed=111,data_source='taxi')\n",
    "train_length, test_length = NYC.init_data()\n",
    "print('train_length:{}'.format(train_length))\n",
    "print('test_length:{}'.format(test_length))\n",
    "\n",
    "TTD_train_loss_history =[]\n",
    "predictions = []\n",
    "test_losses = []\n",
    "for _ in range(10):\n",
    "    TDNet = TemporalDynamicNetwork(data_size=2,layer_size=32,period_length=7,term_length=5,batch_size=64,PSAM=True,cuda=True)\n",
    "    total_train_batches = 128\n",
    "    episode = 500\n",
    "    TDNet_optimizer = torch.optim.Adam(TDNet.parameters(), lr=1e-3)\n",
    "    TD_train_loss = []\n",
    "    for epi in range(episode):\n",
    "        print('current episode: {}'.format(epi))\n",
    "        train,_ = Train(total_train_batches,TDNet,TDNet_optimizer,NYC)\n",
    "        TD_train_loss.append(np.sqrt(train))\n",
    "        \n",
    "    test,pred = Test(9,TDNet,TDNet_optimizer,NYC)\n",
    "    test_losses.append(test)\n",
    "    predictions.append(pred)\n",
    "    \n",
    "    TTD_train_loss_history.append(TD_train_loss)\n",
    "    plt.plot(np.arange(500),TD_train_loss)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb906070",
   "metadata": {},
   "outputs": [],
   "source": [
    "NYC = NYCLoader(batch_size=64,period_length=7,term_length=5,interval_length=7,seed=111,data_source='taxi')\n",
    "train_length, test_length = NYC.init_data()\n",
    "print('train_length:{}'.format(train_length))\n",
    "print('test_length:{}'.format(test_length))\n",
    "\n",
    "TT_train_loss_history =[]\n",
    "predictions = []\n",
    "test_losses = []\n",
    "for _ in range(10):\n",
    "    TNet = TemporalDynamicNetwork(data_size=2,layer_size=32,period_length=7,term_length=5,batch_size=64,PSAM=False,cuda=True)\n",
    "    total_train_batches = 128\n",
    "    episode = 500\n",
    "    TNet_optimizer = torch.optim.Adam(TNet.parameters(), lr=1e-3)\n",
    "    T_train_loss = []\n",
    "    for epi in range(episode):\n",
    "        print('current episode: {}'.format(epi))\n",
    "        train,_ = Train(total_train_batches,TNet,TNet_optimizer,NYC)\n",
    "        T_train_loss.append(np.sqrt(train))\n",
    "        \n",
    "    test,pred = Test(9,TNet,TNet_optimizer,NYC)\n",
    "    test_losses.append(test)\n",
    "    predictions.append(pred)\n",
    "    \n",
    "    TT_train_loss_history.append(T_train_loss)\n",
    "    plt.plot(np.arange(500),T_train_loss)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a3a4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "NYC = NYCLoader(batch_size=64,period_length=7,term_length=5,interval_length=7,seed=1111,data_source='bike')\n",
    "train_length, test_length = NYC.init_data()\n",
    "print('train_length:{}'.format(train_length))\n",
    "print('test_length:{}'.format(test_length))\n",
    "\n",
    "TTB_train_loss_history =[]\n",
    "predictions = []\n",
    "test_losses = []\n",
    "for _ in range(10):\n",
    "    TDNet = TemporalDynamicNetwork(data_size=2,layer_size=32,period_length=7,term_length=5,batch_size=64,PSAM=True,cuda=True)\n",
    "    total_train_batches = 128\n",
    "    episode = 500\n",
    "    TDNet_optimizer = torch.optim.Adam(TDNet.parameters(), lr=1e-3)\n",
    "    TD_train_loss = []\n",
    "    for epi in range(episode):\n",
    "        print('current episode: {}'.format(epi))\n",
    "        train,_ = Train(total_train_batches,TDNet,TDNet_optimizer,NYC)\n",
    "        TD_train_loss.append(np.sqrt(train))\n",
    "        \n",
    "    test,pred = Test(9,TDNet,TDNet_optimizer,NYC)\n",
    "    test_losses.append(test)\n",
    "    predictions.append(pred)\n",
    "    \n",
    "    TTB_train_loss_history.append(TD_train_loss)\n",
    "    plt.plot(np.arange(500),TD_train_loss)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1ccc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NYC = NYCLoader(batch_size=64,period_length=7,term_length=5,interval_length=7,seed=1111,data_source='bike')\n",
    "train_length, test_length = NYC.init_data()\n",
    "print('train_length:{}'.format(train_length))\n",
    "print('test_length:{}'.format(test_length))\n",
    "\n",
    "TB_train_loss_history =[]\n",
    "predictions = []\n",
    "test_losses = []\n",
    "for _ in range(10):\n",
    "    TNet = TemporalDynamicNetwork(data_size=2,layer_size=32,period_length=7,term_length=5,batch_size=64,PSAM=False,cuda=True)\n",
    "    total_train_batches = 128\n",
    "    episode = 500\n",
    "    TNet_optimizer = torch.optim.Adam(TNet.parameters(), lr=1e-3)\n",
    "    T_train_loss = []\n",
    "    for epi in range(episode):\n",
    "        print('current episode: {}'.format(epi))\n",
    "        train,_ = Train(total_train_batches,TNet,TNet_optimizer,NYC)\n",
    "        T_train_loss.append(np.sqrt(train))\n",
    "        \n",
    "    test,pred = Test(9,TNet,TNet_optimizer,NYC)\n",
    "    test_losses.append(test)\n",
    "    predictions.append(pred)\n",
    "    \n",
    "    TB_train_loss_history.append(T_train_loss)\n",
    "    plt.plot(np.arange(500),T_train_loss)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89c356d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ele = ElectricityLoader(batch_size=64,period_length=7,term_length=5,interval_length=7,seed=1111)\n",
    "train_length, test_length = Ele.init_data()\n",
    "print('train_length:{}'.format(train_length))\n",
    "print('test_length:{}'.format(test_length))\n",
    "\n",
    "TTE_train_loss_history =[]\n",
    "predictions = []\n",
    "test_losses = []\n",
    "for _ in range(10):\n",
    "    TDNet = TemporalDynamicNetwork(data_size=1,layer_size=32,period_length=7,term_length=5,batch_size=64,PSAM=True,cuda=True)\n",
    "    total_train_batches = 128\n",
    "    episode = 500\n",
    "    TDNet_optimizer = torch.optim.Adam(TDNet.parameters(), lr=1e-3)\n",
    "    TD_train_loss = []\n",
    "    for epi in range(episode):\n",
    "        print('current episode: {}'.format(epi))\n",
    "        train,_ = Train(total_train_batches,TDNet,TDNet_optimizer,Ele)\n",
    "        TD_train_loss.append(np.sqrt(train))\n",
    "        \n",
    "    test,pred = Test(38,TDNet,TDNet_optimizer,Ele)\n",
    "    test_losses.append(test)\n",
    "    predictions.append(pred)\n",
    "    \n",
    "    TTE_train_loss_history.append(TD_train_loss)\n",
    "    plt.plot(np.arange(500),TD_train_loss)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2c75fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ele = ElectricityLoader(batch_size=64,period_length=7,term_length=5,interval_length=7,seed=1111)\n",
    "train_length, test_length = Ele.init_data()\n",
    "print('train_length:{}'.format(train_length))\n",
    "print('test_length:{}'.format(test_length))\n",
    "\n",
    "TE_train_loss_history =[]\n",
    "predictions = []\n",
    "test_losses = []\n",
    "for _ in range(10):\n",
    "    TNet = TemporalDynamicNetwork(data_size=1,layer_size=32,period_length=7,term_length=5,batch_size=64,PSAM=False,cuda=True)\n",
    "    total_train_batches = 128\n",
    "    episode = 500\n",
    "    TNet_optimizer = torch.optim.Adam(TNet.parameters(), lr=1e-3)\n",
    "    T_train_loss = []\n",
    "    for epi in range(episode):\n",
    "        print('current episode: {}'.format(epi))\n",
    "        train,_ = Train(total_train_batches,TNet,TNet_optimizer,Ele)\n",
    "        T_train_loss.append(np.sqrt(train))\n",
    "        \n",
    "    test,pred = Test(38,TNet,TNet_optimizer,Ele)\n",
    "    test_losses.append(test)\n",
    "    predictions.append(pred)\n",
    "    \n",
    "    TE_train_loss_history.append(T_train_loss)\n",
    "    plt.plot(np.arange(500),T_train_loss)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb283a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Solar = SolarLoader(batch_size=64,period_length=7,term_length=5,interval_length=7,seed=547)\n",
    "train_length, test_length = Solar.init_data()\n",
    "print('train_length:{}'.format(train_length))\n",
    "print('test_length:{}'.format(test_length))\n",
    "\n",
    "TTS_train_loss_history =[]\n",
    "predictions = []\n",
    "test_losses = []\n",
    "for _ in range(10):\n",
    "    TDNet = TemporalDynamicNetwork(data_size=1,layer_size=32,period_length=7,term_length=5,batch_size=64,PSAM=True,cuda=True)\n",
    "    total_train_batches = 128\n",
    "    episode = 500\n",
    "    TDNet_optimizer = torch.optim.Adam(TDNet.parameters(), lr=1e-3)\n",
    "    TD_train_loss = []\n",
    "    for epi in range(episode):\n",
    "        print('current episode: {}'.format(epi))\n",
    "        train,_ = Train(total_train_batches,TDNet,TDNet_optimizer,Solar)\n",
    "        TD_train_loss.append(np.sqrt(train))\n",
    "        \n",
    "    test,pred = Test(76,TDNet,TDNet_optimizer,Solar)\n",
    "    test_losses.append(test)\n",
    "    predictions.append(pred)\n",
    "    \n",
    "    TTS_train_loss_history.append(TD_train_loss)\n",
    "    plt.plot(np.arange(500),TD_train_loss)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bc6572",
   "metadata": {},
   "outputs": [],
   "source": [
    "Solar = SolarLoader(batch_size=64,period_length=7,term_length=5,interval_length=7,seed=547)\n",
    "train_length, test_length = Solar.init_data()\n",
    "print('train_length:{}'.format(train_length))\n",
    "print('test_length:{}'.format(test_length))\n",
    "\n",
    "TS_train_loss_history =[]\n",
    "predictions = []\n",
    "test_losses = []\n",
    "for _ in range(10):\n",
    "    TNet = TemporalDynamicNetwork(data_size=1,layer_size=32,period_length=7,term_length=5,batch_size=64,PSAM=False,cuda=True)\n",
    "    total_train_batches = 128\n",
    "    episode = 500\n",
    "    TNet_optimizer = torch.optim.Adam(TNet.parameters(), lr=1e-3)\n",
    "    T_train_loss = []\n",
    "    for epi in range(episode):\n",
    "        print('current episode: {}'.format(epi))\n",
    "        train,_ = Train(total_train_batches,TNet,TNet_optimizer,Solar)\n",
    "        T_train_loss.append(np.sqrt(train))\n",
    "        \n",
    "    test,pred = Test(76,TNet,TNet_optimizer,Solar)\n",
    "    test_losses.append(test)\n",
    "    predictions.append(pred)\n",
    "    \n",
    "    TS_train_loss_history.append(T_train_loss)\n",
    "    plt.plot(np.arange(500),T_train_loss)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
